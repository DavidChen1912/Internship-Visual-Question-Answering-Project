import os
import gc
import pickle
import pandas as pd
from PIL import Image
import torch
from transformers import AutoModelForCausalLM
from deepseek_vl2.models import DeepseekVLV2Processor, DeepseekVLV2ForCausalLM

def run_inference(model_name, data_type, prompt_name, output_name):
    # ----------------------------
    # è‡ªå‹•å°æ‡‰è·¯å¾‘
    # ----------------------------
    model_path = os.path.join("model", model_name)
    label_path = os.path.join("label", f"{data_type}.pkl")
    image_dir = os.path.join("data", data_type)
    prompt_path = os.path.join("prompt", f"{prompt_name}.txt")
    output_path = os.path.join("outputs", data_type, f"{output_name}.pkl")

    # ----------------------------
    # è¼‰å…¥æ¨¡å‹èˆ‡è™•ç†å™¨
    # ----------------------------
    print(f"ğŸ”§ Loading model from: {model_path}")
    processor = DeepseekVLV2Processor.from_pretrained(model_path)
    tokenizer = processor.tokenizer
    model = DeepseekVLV2ForCausalLM.from_pretrained(model_path, trust_remote_code=True)
    model = model.to(torch.bfloat16).cuda().eval()

    # ----------------------------
    # è¼‰å…¥ prompt
    # ----------------------------
    with open(prompt_path, "r", encoding="utf-8") as f:
        prompt_template = f.read()

    # ----------------------------
    # è¼‰å…¥ label.pkl
    # ----------------------------
    with open(label_path, "rb") as f:
        df = pickle.load(f)

    if "mllm_result" not in df.columns:
        df["mllm_result"] = ""

    # ----------------------------
    # åœ–ç‰‡é è™•ç†
    # ----------------------------
    def preprocess_image(image_path, max_size=1440):
        image = Image.open(image_path).convert("RGB")
        width, height = image.size
        if max(width, height) > max_size:
            ratio = max_size / max(width, height)
            new_width = int(width * ratio)
            new_height = int(height * ratio)
            image = image.resize((new_width, new_height), Image.LANCZOS)
        return image

    # ----------------------------
    # æ¨ç†ä¸»è¿´åœˆ
    # ----------------------------
    for idx, row in df.iterrows():
        image_path = os.path.join(image_dir, row["filename"])
        image = preprocess_image(image_path)

        # æº–å‚™ conversation prompt
        conversation = [
            {"role": "<|User|>", "content": f"<|image|>\n{prompt_template}", "images": [image]},
            {"role": "<|Assistant|>", "content": ""},
        ]

        # ç·¨ç¢¼æˆæ¨¡å‹è¼¸å…¥
        inputs = processor(
            conversations=conversation,
            images=[image],
            force_batchify=True,
            system_prompt=""
        ).to(model.device)

        # æ¨¡å‹æ¨ç†
        inputs_embeds = model.prepare_inputs_embeds(**inputs)
        outputs = model.language_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=inputs.attention_mask,
            bos_token_id=tokenizer.bos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            max_new_tokens=512,
            do_sample=False,
            use_cache=True
        )

        result = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True).strip()
        df.at[idx, "mllm_result"] = result
        print(f"[{idx}] {row['filename']} done.")

        torch.cuda.empty_cache()
        gc.collect()

    # ----------------------------
    # å„²å­˜çµæœ
    # ----------------------------
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "wb") as f:
        pickle.dump(df, f)

    print(f"\n æ¨ç†å®Œæˆï¼çµæœå„²å­˜æ–¼ï¼š{output_path}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, required=True, help="æ¨¡å‹åç¨±ï¼Œä¾‹å¦‚ deepseek-vl2-tinyï¼ˆå°æ‡‰ model/{model}ï¼‰")
    parser.add_argument("--data_type", type=str, required=True, help="è³‡æ–™é¡å‹ï¼Œä¾‹å¦‚ æç›Šè¡¨ã€è²¸æ¬¾ç”³è«‹æ›¸ï¼ˆå°æ‡‰ label/{data_type}.pkl å’Œ data/{data_type}/ï¼‰")
    parser.add_argument("--prompt_name", type=str, required=True, help="prompt æª”åï¼ˆä¸å«å‰¯æª”åï¼Œå°æ‡‰ prompt/{prompt_name}.txtï¼‰")
    parser.add_argument("--output_name", type=str, required=True, help="output çš„æª”åï¼ˆä¸å«å‰¯æª”åï¼Œå°‡å„²å­˜è‡³ outputs/{data_type}/{output_name}.pklï¼‰")
    args = parser.parse_args()

    run_inference(
        model_name=args.model,
        data_type=args.data_type,
        prompt_name=args.prompt_name,
        output_name=args.output_name
    )

# python inference.py --model=deepseek-vl2-tiny --data_type=æç›Šè¡¨ --prompt_name=æç›Šè¡¨_v3 --output_name=DeepSeek-VL2-test
